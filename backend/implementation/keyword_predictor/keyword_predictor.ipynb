{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-GxEodApJUy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/target-guided-sat-chatbot\")"
      ],
      "metadata": {
        "id": "h52LbIqCcoNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "68woVQdZg-2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx, from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "import torchtext\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from nltk import word_tokenize\n",
        "import json\n",
        "from global_planning import GlobalPlanning\n",
        "import pickle"
      ],
      "metadata": {
        "id": "5QKrwkZfcytn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "OidJqrU_x-vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keyword Predictor Model"
      ],
      "metadata": {
        "id": "ATLOOgKgZcGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KeywordPredictor(nn.Module):\n",
        "    def __init__(self, global_planning):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        print(f\"Using Cuda?: {torch.cuda.is_available()}\")\n",
        "        self.embed_size = 300\n",
        "        self.context_enc_hidden_size = 256\n",
        "        self.global_planning = global_planning\n",
        "        self.glove = self.global_planning.glove\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.embed_size + self.embed_size + self.context_enc_hidden_size * 2, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        ).to(self.device)\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(self.glove.vectors).to(self.device)\n",
        "        self.gcn_graph_encoder = GCN(self.embed_size).to(self.device)\n",
        "        self.context_encoder = nn.GRU(self.embed_size, self.context_enc_hidden_size, 1, bidirectional=True).to(self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        # self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, nodes_ids, global_graphs, target_ids, context_embeddings, labels):\n",
        "        loss = None\n",
        "        logits_list = []\n",
        "        for node_id, global_graph, target_id, context_embedding, label in zip(nodes_ids, global_graphs, target_ids, context_embeddings, labels):\n",
        "            inputs = torch.cat((global_graph, target_id, context_embedding), 1)\n",
        "            logits = self.fc(inputs) # logits: (vocab_size, 2)\n",
        "            logits_list.append(logits)\n",
        "\n",
        "            if loss is None:\n",
        "                loss = self.criterion(logits, label) # labels: (vocab_size)\n",
        "            else:\n",
        "                loss += self.criterion(logits, label)\n",
        "        return {'loss': loss, 'logits': logits_list}\n",
        "\n",
        "    def predict(self, nodes_ids, global_graphs, target_ids, context_embeddings):\n",
        "        self.eval()\n",
        "        logits_list = []\n",
        "        for node_id, global_graph, target_id, context_embedding in zip(nodes_ids, global_graphs, target_ids, context_embeddings):\n",
        "            inputs = torch.cat((global_graph, target_id, context_embedding), 1)\n",
        "            logits = self.fc(inputs) # logits: (vocab_size, 2)\n",
        "            logits_list.append(logits)\n",
        "\n",
        "        return {'logits': logits_list}\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        global_graphs_nodes_ids = [b['global_graphs'].x.to(self.device) for b in batch]\n",
        "\n",
        "        global_graphs_embeddings = [self.gcn_graph_encoder(self.embedding_layer(b['global_graphs'].x.to(self.device)), b['global_graphs'].edge_index.to(self.device)) for b in batch]\n",
        "\n",
        "        target_ids_embeddings = [self.embedding_layer(torch.tensor(b['target_ids']).to(self.device)).mean(0).repeat(b['global_graphs'].num_nodes, 1) for b in batch]\n",
        "\n",
        "        context_ids_embeddings = [self.context_encoder(self.embedding_layer(torch.tensor(b['context_ids']).to(self.device)))[1].reshape(-1).repeat(b['global_graphs'].num_nodes, 1) for b in batch]\n",
        "\n",
        "        labels = [b['labels'].to(self.device) for b in batch]\n",
        "        return {'nodes_ids':global_graphs_nodes_ids, 'global_graphs':global_graphs_embeddings, 'target_ids':target_ids_embeddings, 'context_embeddings':context_ids_embeddings, 'labels':labels}\n",
        "\n",
        "# GCN to encode graph\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(embed_size, 512)\n",
        "        self.conv2 = GCNConv(512, embed_size)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gBxP-eqgZKvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "1C0MnVMBZfXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "#               Dataset\n",
        "# ========================================\n",
        "\n",
        "class KeywordPredictionDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_json, global_planning, topk_num):\n",
        "\n",
        "    self.context_ids = []\n",
        "    self.global_graphs = []\n",
        "    self.target_ids = []\n",
        "    self.labels = []\n",
        "    self.global_planning = global_planning\n",
        "    self.glove = self.global_planning.glove\n",
        "    self.topk_num = topk_num\n",
        "\n",
        "    max_context_length = 3\n",
        "\n",
        "    for data in tqdm(data_json):\n",
        "\n",
        "        dialog = data['dialog']\n",
        "        concepts = data['concepts']\n",
        "\n",
        "        idx = random.randint(1, len(dialog) - 2)\n",
        "        start_idx = max(0, idx - max_context_length)\n",
        "        context = dialog[start_idx:idx]\n",
        "        target = dialog[idx + 1]\n",
        "\n",
        "        # TODO: check concept in wv and graph\n",
        "        start_concept = [c for c in concepts[idx - 1] if self.global_planning.word_exists_in_conceptnet(c) and self.global_planning.word_embedding_exists(c)]\n",
        "        bridge_concept = [c for c in concepts[idx] if self.global_planning.word_exists_in_conceptnet(c) and self.global_planning.word_embedding_exists(c)]\n",
        "        target_concept = [c for c in concepts[idx + 1] if self.global_planning.word_exists_in_conceptnet(c) and self.global_planning.word_embedding_exists(c)]\n",
        "\n",
        "        # TODO: check bc and tc is not empty\n",
        "        if len(start_concept) == 0 or len(bridge_concept) == 0 or len(target_concept) == 0:\n",
        "           continue\n",
        "\n",
        "        # Context\n",
        "        context_id = []\n",
        "        for dialog in context:\n",
        "          for word in word_tokenize(dialog):\n",
        "            if word in self.glove.stoi:\n",
        "              context_id.append(self.glove.stoi[word])\n",
        "\n",
        "        # Global graph\n",
        "        global_graph = nx.Graph()\n",
        "        for s in start_concept:\n",
        "            for t in target_concept:\n",
        "                self.global_planning.find_path(s, t, global_graph)\n",
        "\n",
        "        global_graph_nodes = list(global_graph.nodes)\n",
        "        if len(global_graph_nodes) < self.topk_num:\n",
        "           continue # not enough nodes\n",
        "\n",
        "        for n in global_graph_nodes:\n",
        "            global_graph.nodes[n]['x'] = self.glove.stoi[n]\n",
        "\n",
        "        # Target\n",
        "        target_id = []\n",
        "        for t in target_concept:\n",
        "          if t in global_graph_nodes:\n",
        "            target_id.append(global_planning.glove.stoi[t])\n",
        "            for n in global_graph.neighbors(t):\n",
        "              target_id.append(global_planning.glove.stoi[n])\n",
        "\n",
        "        # Label of classification task\n",
        "        node_to_idx = dict(zip(global_graph_nodes, range(len(global_graph_nodes))))\n",
        "        bridge_idxs = [node_to_idx[n] for n in bridge_concept if n in node_to_idx]\n",
        "        if len(bridge_idxs) == 0:\n",
        "           print(\"no true labels\", bridge_concept)\n",
        "           continue # no true labels\n",
        "\n",
        "        candidate_nodes = set()\n",
        "        # limited to 2 hops\n",
        "        for c0 in start_concept:\n",
        "          if c0 in global_graph_nodes:\n",
        "            candidate_nodes.add(c0)\n",
        "            for c1 in global_graph.neighbors(c0):\n",
        "              if c1 in global_graph_nodes:\n",
        "                candidate_nodes.add(c1)\n",
        "                for c2 in global_graph.neighbors(c1):\n",
        "                    candidate_nodes.add(c2)\n",
        "        candidate_idxs = [node_to_idx[n] for n in candidate_nodes]\n",
        "\n",
        "        label = torch.ones(len(global_graph_nodes), dtype=int) * -100\n",
        "        label[candidate_idxs] = 0\n",
        "        label[bridge_idxs] = 1\n",
        "\n",
        "        self.context_ids.append(context_id)\n",
        "        self.global_graphs.append(from_networkx(global_graph)) # Converts the graph to a torch_geometric.data.Data instance.\n",
        "        self.target_ids.append(target_id)\n",
        "        self.labels.append(label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.context_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {'context_ids' : self.context_ids[idx], 'global_graphs' : self.global_graphs[idx], 'target_ids' : self.target_ids[idx], 'labels' : self.labels[idx]}\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "R-UuWMbnZWQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "2WA6bg89ZjfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "metadata": {
        "id": "s5VjJUl0Z2SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_numberbatch = False\n",
        "global_planning = GlobalPlanning(use_numberbatch) # around 2 min"
      ],
      "metadata": {
        "id": "9qKx30ymZ-fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce dataset:"
      ],
      "metadata": {
        "id": "_LZuSc0E4j-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/train/concepts_nv.json') as f:\n",
        "    train_data_json = [json.loads(row) for row in f]\n",
        "train_data_json = train_data_json[:10]\n",
        "print(f\"train length: {len(train_data_json)}\")"
      ],
      "metadata": {
        "id": "Z9okohrYZ4ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topk_num = 5\n",
        "train_dataset = KeywordPredictionDataset(train_data_json, global_planning, topk_num)\n",
        "print(\"Train dataset length: \", len(train_dataset))"
      ],
      "metadata": {
        "id": "v41iIxGCqOGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle dataset\n",
        "with open(\"keyword_predictor_train_dataset.pickle\", \"wb\") as f:\n",
        "    pickle.dump(train_dataset, f)"
      ],
      "metadata": {
        "id": "twBwcN71qR4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size=batch_size, collate_fn=keyword_predictor_model.collate_fn)\n",
        "# print(next(iter(train_dataloader)))"
      ],
      "metadata": {
        "id": "bhRh9uLEx3b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use pickled data"
      ],
      "metadata": {
        "id": "hB0SWu2e4fHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open pickled dataset\n",
        "with open(\"keyword_prediction_dataset/keyword_predictor_train_dataset.pickle\", \"rb\") as f:\n",
        "    train_dataset_pickled = pickle.load(f)"
      ],
      "metadata": {
        "id": "Y5Dxfy2Up2OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"keyword_prediction_dataset/keyword_predictor_val_dataset.pickle\", \"rb\") as f:\n",
        "    val_dataset_pickled = pickle.load(f)"
      ],
      "metadata": {
        "id": "S-q2gSU0ijDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_predictor_model.to('cpu')\n",
        "keyword_predictor_model = KeywordPredictor(global_planning)\n",
        "keyword_predictor_model.to(keyword_predictor_model.device)"
      ],
      "metadata": {
        "id": "gXnb1rklaERP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset_pickled, sampler = RandomSampler(train_dataset_pickled), batch_size=batch_size, collate_fn=keyword_predictor_model.collate_fn)\n",
        "validation_dataloader = DataLoader(val_dataset_pickled, sampler = SequentialSampler(val_dataset_pickled), batch_size=batch_size, collate_fn=keyword_predictor_model.collate_fn)"
      ],
      "metadata": {
        "id": "dgax723VqZLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "num_training_steps = len(train_dataloader) * num_epochs\n",
        "optimizer = optim.Adam(keyword_predictor_model.parameters(), lr=1e-6)"
      ],
      "metadata": {
        "id": "kZCwJTJBBm5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "li0DQaUN3ygf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_t0 = time.time()\n",
        "training_stats = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    keyword_predictor_model.train()\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = keyword_predictor_model(**batch)\n",
        "        loss = torch.div(outputs['loss'], batch_size)\n",
        "        logits = outputs['logits']\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        # Get sample every x batches.\n",
        "        if step % 50 == 0:    # print every 100 mini-batches\n",
        "          elapsed = format_time(time.time() - t0)\n",
        "          print('\\n  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), loss, elapsed))\n",
        "          # print('logits: ', logits[0])\n",
        "          all_preds_idx = (logits[0].softmax(1).argmax(1)==1).nonzero().flatten()\n",
        "          preds_ones = logits[0].softmax(1)[:, 1]\n",
        "          _, topk_preds_idx_1 = logits[0].softmax(1)[:, 1].topk(5)\n",
        "          _, topk_preds_idx_0 = logits[0].softmax(1)[:, 0].topk(5)\n",
        "          topk = batch['nodes_ids'][0][topk_preds_idx_1]\n",
        "          preds = batch['nodes_ids'][0][all_preds_idx]\n",
        "          candidates = batch['nodes_ids'][0][batch['labels'][0] == 0]\n",
        "          true = batch['nodes_ids'][0][batch['labels'][0] == 1]\n",
        "\n",
        "          topk_pred_words = [keyword_predictor_model.global_planning.glove.itos[id] for id in topk]\n",
        "          print('Predicted: ', [keyword_predictor_model.global_planning.glove.itos[id] for id in preds])\n",
        "          print('Softmax: ', preds_ones[topk_preds_idx_1], preds_ones[topk_preds_idx_0])\n",
        "          print('Top 5 predictions: ', topk_pred_words)\n",
        "          print([keyword_predictor_model.global_planning.glove.itos[id] for id in candidates])\n",
        "          predicted_candidates_id = [id for id in topk if id in candidates]\n",
        "          predicted_true_id = [id for id in topk if id in true]\n",
        "          print('True labels: ', [keyword_predictor_model.global_planning.glove.itos[id] for id in true])\n",
        "          print('Predicted candidates: ', len(predicted_candidates_id), [keyword_predictor_model.global_planning.glove.itos[id] for id in predicted_candidates_id])\n",
        "          print('True candidates: ', len(predicted_true_id), [keyword_predictor_model.global_planning.glove.itos[id] for id in predicted_true_id])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    keyword_predictor_model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    for step, batch in enumerate(tqdm(validation_dataloader)):\n",
        "      loss = torch.div(keyword_predictor_model(**batch)['loss'], batch_size)\n",
        "      total_eval_loss += loss.item()\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n"
      ],
      "metadata": {
        "id": "CUnughFKaH_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_predictor_model.to('cpu')\n",
        "model_state_dict_path = \"keyword_predictor_state_dict_model_8_10_1e-6.pt\"\n",
        "torch.save(keyword_predictor_model.state_dict(), model_state_dict_path)\n",
        "\"\"\"\n",
        "To load:\n",
        "the_model = TheModelClass(*args, **kwargs)\n",
        "the_model.load_state_dict(torch.load(PATH))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SajaSXfKBOU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Display floats with two decimal places.\n",
        "# pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "id": "ASfPZim6iTd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"predictor_stats.pickle\", \"wb\") as f:\n",
        "  pickle.dump(training_stats, f)"
      ],
      "metadata": {
        "id": "IFNu25o7iZIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot styling from seaborn.\n",
        "# sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "# sns.set(font_scale=1.5)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks(range(1, num_epochs + 1))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E8KrkYzRiWWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict_model = KeywordPredictor(global_planning)\n",
        "state_dict_model.load_state_dict(torch.load(model_state_dict_path))\n",
        "state_dict_model.eval() # must be set before inference"
      ],
      "metadata": {
        "id": "QejWupZh4vfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}