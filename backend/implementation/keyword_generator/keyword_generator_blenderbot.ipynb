{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TawkRne1YOd6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S89lPGWoQM1D"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyQpnHSrYd3L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/target-guided-sat-chatbot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ynv_dRiRSN-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE7D_ErjFYoD"
      },
      "outputs": [],
      "source": [
        "mname = \"facebook/blenderbot-400M-distill\"\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(mname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ZRJWmjFk-i"
      },
      "outputs": [],
      "source": [
        "special_tokens = {'additional_special_tokens': [\"<keyword>\"]}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "print(tokenizer.all_special_tokens) # --> ['<s>', '</s>', '<unk>', '<pad>', '<mask>', '<keyword>']\n",
        "print(tokenizer.all_special_ids)    # --> [1, 2, 3, 0, 8008, 8009]\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load raw data"
      ],
      "metadata": {
        "id": "4A-94Jv0WxbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8f3OMk-ZFpy"
      },
      "outputs": [],
      "source": [
        "with open('data/train/concepts_nv.json') as f:\n",
        "  train_data_json = [json.loads(row) for row in f]\n",
        "print(f\"train length: {len(train_data_json)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JADn4sqpTNQo"
      },
      "outputs": [],
      "source": [
        "with open('data/dev/concepts_nv.json') as f:\n",
        "  validation_data_json = [json.loads(row) for row in f]\n",
        "print(f\"validation length: {len(validation_data_json)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class"
      ],
      "metadata": {
        "id": "aqqUoW8lNjC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KeywordGenerationDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_json, tokenizer):\n",
        "\n",
        "    self.input_ids = []\n",
        "    self.labels = []\n",
        "\n",
        "    bos = tokenizer.bos_token\n",
        "    eos = tokenizer.eos_token\n",
        "    kw_token = tokenizer.additional_special_tokens[0]\n",
        "\n",
        "    max_input_length = 128\n",
        "    max_context_length = 5\n",
        "\n",
        "    for data in tqdm(data_json):\n",
        "      dialog = data['dialog']\n",
        "      concepts = data['concepts']\n",
        "\n",
        "      for idx in range(1, len(dialog)):\n",
        "\n",
        "        keywords = concepts[idx]\n",
        "\n",
        "        start_idx = max(0, idx-max_context_length+1)\n",
        "        contexts = dialog[start_idx:idx]\n",
        "        response = dialog[idx]\n",
        "\n",
        "        encoded_contexts = [tokenizer.encode(bos + c + eos, add_special_tokens=False) for c in contexts]\n",
        "\n",
        "        random.shuffle(keywords)\n",
        "        keywords_with_special_tokens = kw_token + kw_token.join(keywords)\n",
        "        encoded_keywords = tokenizer.encode(keywords_with_special_tokens, add_special_tokens=False)\n",
        "\n",
        "        input_ids = encoded_keywords + list(chain.from_iterable(encoded_contexts))\n",
        "        assert len(input_ids) <= max_input_length\n",
        "\n",
        "        labels = tokenizer.encode(response)\n",
        "\n",
        "        self.input_ids.append(input_ids)\n",
        "        self.labels.append(labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {'input_ids' : self.input_ids[idx], 'labels' : self.labels[idx]}\n",
        "\n"
      ],
      "metadata": {
        "id": "WKw_eXvmNnGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  input_ids, labels = [], []\n",
        "  eos_id = tokenizer.eos_token_id\n",
        "  pad_id = tokenizer.pad_token_id\n",
        "  for b in batch:\n",
        "    input_ids.append(torch.LongTensor(b['input_ids']))\n",
        "    labels.append(torch.LongTensor(b['labels']))\n",
        "\n",
        "  input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=pad_id)\n",
        "  labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "  return {'input_ids' : input_ids, 'labels' : labels}"
      ],
      "metadata": {
        "id": "IH_vMaDDXXDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdyKJrsffDD7"
      },
      "outputs": [],
      "source": [
        "train_dataset = KeywordGenerationDataset(train_data_json, tokenizer)\n",
        "validation_dataset = KeywordGenerationDataset(validation_data_json, tokenizer)\n",
        "print(len(validation_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save pickled dataset\n",
        "with open(\"keyword_generation_dataset/generation_train_dataset_blenderbot.pickle\", \"wb\") as f:\n",
        "    pickle.dump(train_dataset, f)\n",
        "with open(\"keyword_generation_dataset/generation_dev_dataset_blenderbot.pickle\", \"wb\") as f:\n",
        "    pickle.dump(validation_dataset, f)"
      ],
      "metadata": {
        "id": "dAk0_xUZn-OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use pickled dataset"
      ],
      "metadata": {
        "id": "undvHkbGPveW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open pickled dataset\n",
        "# with open(\"keyword_generation_dataset/generation_train_dataset_gpt2.pickle\", \"rb\") as f:\n",
        "#     train_dataset = pickle.load(f)\n",
        "# with open(\"keyword_generation_dataset/generation_dev_dataset_gpt2.pickle\", \"rb\") as f:\n",
        "#     validation_dataset = pickle.load(f)"
      ],
      "metadata": {
        "id": "fjjcmzBAN2Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rae3uSXf9TV"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size=batch_size, collate_fn=collate_fn)\n",
        "validation_dataloader = DataLoader(validation_dataset, sampler = SequentialSampler(validation_dataset), batch_size=batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxcIf-1eBkGO"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "num_training_steps = len(train_dataloader) * num_epochs # Total number of training steps is [number of batches] x [number of epochs].\n",
        "learning_rate = 2e-5 # 5e-4\n",
        "warmup_ratio = 0.1\n",
        "warmup_steps = int(warmup_ratio * num_training_steps ) # 1e2\n",
        "\n",
        "# this produces sample output every 100 steps\n",
        "sample_step = 100\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                )\n",
        "\n",
        "# lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "#                                             num_warmup_steps = warmup_steps,\n",
        "#                                             num_training_steps = num_training_steps)\n",
        "\n",
        "lr_scheduler = get_polynomial_decay_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=warmup_steps,\n",
        "                num_training_steps=num_training_steps,\n",
        "                power=2\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGcvWjkzB8Lq"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxSvZlwZCX1z"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using Cuda?: {torch.cuda.is_available()}\")\n",
        "model.to(device)\n",
        "\n",
        "total_t0 = time.time()\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "training_stats = []\n",
        "best_loss = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
        "\n",
        "  # ========================================\n",
        "  #               Training\n",
        "  # ========================================\n",
        "  print('Training...')\n",
        "  t0 = time.time()\n",
        "  total_train_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids=b_input_ids,\n",
        "                    labels=b_labels\n",
        "                    )\n",
        "    loss = outputs.loss\n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # Get sample every x batches.\n",
        "    if step % sample_step == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), loss, elapsed))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    progress_bar.update(1)\n",
        "  # Calculate the average loss over all of the batches.\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "  # Measure how long this epoch took.\n",
        "  training_time = format_time(time.time() - t0)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "  # ========================================\n",
        "  #               Validation\n",
        "  # ========================================\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Running Validation...\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_eval_loss = 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids=b_input_ids,\n",
        "                    labels=b_labels\n",
        "                    )\n",
        "    loss = outputs.loss\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "  avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "  validation_time = format_time(time.time() - t0)\n",
        "\n",
        "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "  print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "  # Record all statistics from this epoch.\n",
        "  training_stats.append(\n",
        "      {\n",
        "          'epoch': epoch + 1,\n",
        "          'Training Loss': avg_train_loss,\n",
        "          'Valid. Loss': avg_val_loss,\n",
        "          'Training Time': training_time,\n",
        "          'Validation Time': validation_time\n",
        "      }\n",
        "  )\n",
        "\n",
        "  # if avg_val_loss < best_loss:\n",
        "  best_loss = avg_val_loss\n",
        "  state_dict = {\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optim_state_dict': optimizer.state_dict(),\n",
        "                    'sched_state_dict': lr_scheduler.state_dict(),\n",
        "                    'loss': best_loss,\n",
        "                    'epoch': epoch + 1\n",
        "                }\n",
        "\n",
        "  torch.save(state_dict, f\"blenderbot_best_ckpt_epoch={epoch+1}_valid_loss={round(best_loss, 4)}.ckpt\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8JisHdjU79u"
      },
      "outputs": [],
      "source": [
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_blenderbot_more_data/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6DJtSzdUxTN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Display floats with two decimal places.\n",
        "# pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-IIUtxmCMmL"
      },
      "outputs": [],
      "source": [
        "with open(\"model_blenderbot_more_data.pickle\", \"wb\") as f:\n",
        "  pickle.dump(training_stats, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8sE_WQoU4sa"
      },
      "outputs": [],
      "source": [
        "# Use plot styling from seaborn.\n",
        "# sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "# sns.set(font_scale=1.5)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}