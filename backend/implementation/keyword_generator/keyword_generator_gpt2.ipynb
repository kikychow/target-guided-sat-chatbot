{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TawkRne1YOd6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S89lPGWoQM1D"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyQpnHSrYd3L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/target-guided-sat-chatbot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7NgwpsGxUL6"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ynv_dRiRSN-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE7D_ErjFYoD"
      },
      "outputs": [],
      "source": [
        "mname = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(mname)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(mname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ZRJWmjFk-i"
      },
      "outputs": [],
      "source": [
        "special_tokens = {\n",
        "    'bos_token': \"<|startoftext|>\",\n",
        "    'additional_special_tokens': [\"<|keyword|>\", \"<|speaker1|>\", \"<|speaker2|>\"]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "print(tokenizer.all_special_tokens) # --> ['<|startoftext|>', '<|endoftext|>', '<|keyword|>', '<|speaker1|>', '<|speaker2|>']\n",
        "print(tokenizer.all_special_ids)    # --> [50261, 50256, 50262, 50263, 50264]\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load raw data"
      ],
      "metadata": {
        "id": "4A-94Jv0WxbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8f3OMk-ZFpy"
      },
      "outputs": [],
      "source": [
        "with open('data/train/concepts_nv.json') as f:\n",
        "  train_data_json = [json.loads(row) for row in f]\n",
        "print(f\"train length: {len(train_data_json)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JADn4sqpTNQo"
      },
      "outputs": [],
      "source": [
        "with open('data/dev/concepts_nv.json') as f:\n",
        "  validation_data_json = [json.loads(row) for row in f]\n",
        "print(f\"validation length: {len(validation_data_json)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class"
      ],
      "metadata": {
        "id": "aqqUoW8lNjC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KeywordGenerationDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_json, tokenizer):\n",
        "\n",
        "    self.input_ids = []\n",
        "    self.token_type_ids = []\n",
        "    self.labels = []\n",
        "\n",
        "    bos_id = tokenizer.bos_token_id\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    speaker1_id = tokenizer.additional_special_tokens_ids[1]\n",
        "    speaker2_id = tokenizer.additional_special_tokens_ids[2]\n",
        "    kw_token = tokenizer.additional_special_tokens[0]\n",
        "\n",
        "    max_input_length = 1024\n",
        "    max_context_length = 5\n",
        "\n",
        "    for data in tqdm(data_json):\n",
        "      dialog = data['dialog']\n",
        "      concepts = data['concepts']\n",
        "\n",
        "      for idx in range(1, len(dialog)):\n",
        "        keywords = concepts[idx]\n",
        "\n",
        "        start_idx = max(0, idx-max_context_length+1)\n",
        "        contexts = dialog[start_idx:idx+1]\n",
        "        if len(contexts) % 2 == 0:\n",
        "          start_speaker_id = speaker1_id\n",
        "          next_speaker_id = speaker2_id\n",
        "        else:\n",
        "          start_speaker_id = speaker2_id\n",
        "          next_speaker_id = speaker1_id\n",
        "        encoded_contexts = [[start_speaker_id] + tokenizer.encode(c) if i % 2 == 0 else [next_speaker_id] + tokenizer.encode(c) for i, c in enumerate(contexts)]\n",
        "        assert encoded_contexts[-1][0] == speaker2_id\n",
        "\n",
        "        random.shuffle(keywords)\n",
        "        keywords_with_special_tokens = kw_token + kw_token.join(keywords)\n",
        "        encoded_keywords = tokenizer.encode(keywords_with_special_tokens)\n",
        "\n",
        "        input_ids = encoded_keywords + [bos_id] + list(chain.from_iterable(encoded_contexts)) + [eos_id]\n",
        "        if len(input_ids) > max_input_length:\n",
        "          continue\n",
        "\n",
        "        token_type_ids_keywords = [speaker2_id] * len(encoded_keywords)\n",
        "        token_type_ids_contexts = [[start_speaker_id] * len(c) if i % 2 == 0 else [next_speaker_id] * len(c) for i, c in enumerate(encoded_contexts)]\n",
        "        assert token_type_ids_contexts[-1][0] == speaker2_id\n",
        "        token_type_ids = token_type_ids_keywords + [speaker1_id] + list(chain.from_iterable(token_type_ids_contexts)) + [speaker2_id]\n",
        "        assert len(input_ids) == len(token_type_ids)\n",
        "\n",
        "        labels = [-100] * (len(encoded_keywords) + 1 + sum([len(c) for c in encoded_contexts[:-1]]) + 1) + encoded_contexts[-1][1:] + [eos_id]\n",
        "        assert len(input_ids) == len(labels)\n",
        "\n",
        "        self.input_ids.append(input_ids)\n",
        "        self.token_type_ids.append(token_type_ids)\n",
        "        self.labels.append(labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {'input_ids' : self.input_ids[idx], 'token_type_ids' : self.token_type_ids[idx], 'labels' : self.labels[idx]}\n",
        "\n"
      ],
      "metadata": {
        "id": "WKw_eXvmNnGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  input_ids, token_type_ids, labels = [], [], []\n",
        "  eos_id = tokenizer.eos_token_id\n",
        "  for b in batch:\n",
        "    input_ids.append(torch.LongTensor(b['input_ids']))\n",
        "    token_type_ids.append(torch.LongTensor(b['token_type_ids']))\n",
        "    labels.append(torch.LongTensor(b['labels']))\n",
        "\n",
        "  input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=eos_id)\n",
        "  token_type_ids = torch.nn.utils.rnn.pad_sequence(token_type_ids, batch_first=True, padding_value=eos_id)\n",
        "  labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "  return {'input_ids' : input_ids, 'token_type_ids' : token_type_ids, 'labels' : labels}"
      ],
      "metadata": {
        "id": "IH_vMaDDXXDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdyKJrsffDD7"
      },
      "outputs": [],
      "source": [
        "train_dataset = KeywordGenerationDataset(train_data_json, tokenizer)\n",
        "validation_dataset = KeywordGenerationDataset(validation_data_json, tokenizer)\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use pickled dataset"
      ],
      "metadata": {
        "id": "undvHkbGPveW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open pickled dataset\n",
        "with open(\"keyword_generation_dataset/generation_train_dataset_gpt2.pickle\", \"rb\") as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "with open(\"keyword_generation_dataset/generation_dev_dataset_gpt2.pickle\", \"rb\") as f:\n",
        "    validation_dataset = pickle.load(f)"
      ],
      "metadata": {
        "id": "fjjcmzBAN2Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rae3uSXf9TV"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size=batch_size, collate_fn=collate_fn)\n",
        "validation_dataloader = DataLoader(validation_dataset, sampler = SequentialSampler(validation_dataset), batch_size=batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxcIf-1eBkGO"
      },
      "outputs": [],
      "source": [
        "# some parameters I cooked up that work reasonably well\n",
        "\n",
        "num_epochs = 5\n",
        "num_training_steps = len(train_dataloader) * num_epochs # Total number of training steps is [number of batches] x [number of epochs].\n",
        "learning_rate = 2e-5 # 5e-4\n",
        "warmup_ratio = 0.1\n",
        "warmup_steps = int(warmup_ratio * num_training_steps ) # 1e2\n",
        "\n",
        "# this produces sample output every 100 steps\n",
        "sample_step = 100\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                )\n",
        "\n",
        "# lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "#                                             num_warmup_steps = warmup_steps,\n",
        "#                                             num_training_steps = num_training_steps)\n",
        "\n",
        "lr_scheduler = get_polynomial_decay_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=warmup_steps,\n",
        "                num_training_steps=num_training_steps,\n",
        "                power=2\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGcvWjkzB8Lq"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxSvZlwZCX1z"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using Cuda?: {torch.cuda.is_available()}\")\n",
        "model.to(device)\n",
        "\n",
        "total_t0 = time.time()\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "training_stats = []\n",
        "best_loss = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
        "\n",
        "  # ========================================\n",
        "  #               Training\n",
        "  # ========================================\n",
        "  print('Training...')\n",
        "  t0 = time.time()\n",
        "  total_train_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_token_type_ids = batch['token_type_ids'].to(device)\n",
        "    b_labels = batch['labels'].to(device)\n",
        "    outputs = model(b_input_ids,\n",
        "                    token_type_ids=b_token_type_ids,\n",
        "                    labels=b_labels\n",
        "                    )\n",
        "    loss = outputs.loss\n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # Get sample every x batches.\n",
        "    if step % sample_step == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), loss, elapsed))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    progress_bar.update(1)\n",
        "  # Calculate the average loss over all of the batches.\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "  # Measure how long this epoch took.\n",
        "  training_time = format_time(time.time() - t0)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "  # ========================================\n",
        "  #               Validation\n",
        "  # ========================================\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Running Validation...\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_eval_loss = 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_token_type_ids = batch['token_type_ids'].to(device)\n",
        "    b_labels = batch['labels'].to(device)\n",
        "    outputs = model(b_input_ids,\n",
        "                    token_type_ids=b_token_type_ids,\n",
        "                    labels=b_labels\n",
        "                    )\n",
        "    loss = outputs.loss\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "  avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "  validation_time = format_time(time.time() - t0)\n",
        "\n",
        "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "  print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "  # Record all statistics from this epoch.\n",
        "  training_stats.append(\n",
        "      {\n",
        "          'epoch': epoch + 1,\n",
        "          'Training Loss': avg_train_loss,\n",
        "          'Valid. Loss': avg_val_loss,\n",
        "          'Training Time': training_time,\n",
        "          'Validation Time': validation_time\n",
        "      }\n",
        "  )\n",
        "\n",
        "  if avg_val_loss < best_loss:\n",
        "    best_loss = avg_val_loss\n",
        "    state_dict = {\n",
        "                      'model_state_dict': model.state_dict(),\n",
        "                      'optim_state_dict': optimizer.state_dict(),\n",
        "                      'sched_state_dict': lr_scheduler.state_dict(),\n",
        "                      'loss': best_loss,\n",
        "                      'epoch': epoch + 1\n",
        "                  }\n",
        "\n",
        "    torch.save(state_dict, f\"best_ckpt_epoch={epoch+1}_valid_loss={round(best_loss, 4)}.ckpt\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = avg_val_loss\n",
        "state_dict = {\n",
        "                  'model_state_dict': model.state_dict(),\n",
        "                  'optim_state_dict': optimizer.state_dict(),\n",
        "                  'sched_state_dict': lr_scheduler.state_dict(),\n",
        "                  'loss': best_loss,\n",
        "                  'epoch': 5\n",
        "              }\n",
        "\n",
        "torch.save(state_dict, f\"best_ckpt_epoch=5_valid_loss={round(best_loss, 4)}.ckpt\")\n"
      ],
      "metadata": {
        "id": "mFJwwB1ZmN_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_t0 = time.time()\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "\n",
        "for epoch in range(5, 10):\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, 10))\n",
        "\n",
        "  # ========================================\n",
        "  #               Training\n",
        "  # ========================================\n",
        "  print('Training...')\n",
        "  t0 = time.time()\n",
        "  total_train_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_token_type_ids = batch['token_type_ids'].to(device)\n",
        "    b_labels = batch['labels'].to(device)\n",
        "    outputs = model(b_input_ids,\n",
        "                    token_type_ids=b_token_type_ids,\n",
        "                    labels=b_labels\n",
        "                    )\n",
        "    loss = outputs.loss\n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # Get sample every x batches.\n",
        "    if step % sample_step == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), loss, elapsed))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    progress_bar.update(1)\n",
        "  # Calculate the average loss over all of the batches.\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "  # Measure how long this epoch took.\n",
        "  training_time = format_time(time.time() - t0)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "  # ========================================\n",
        "  #               Validation\n",
        "  # ========================================\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Running Validation...\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_eval_loss = 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_token_type_ids = batch['token_type_ids'].to(device)\n",
        "    b_labels = batch['labels'].to(device)\n",
        "    outputs = model(b_input_ids,\n",
        "                    token_type_ids=b_token_type_ids,\n",
        "                    labels=b_labels\n",
        "                    )\n",
        "    loss = outputs.loss\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "  avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "  validation_time = format_time(time.time() - t0)\n",
        "\n",
        "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "  print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "  # Record all statistics from this epoch.\n",
        "  training_stats.append(\n",
        "      {\n",
        "          'epoch': epoch + 1,\n",
        "          'Training Loss': avg_train_loss,\n",
        "          'Valid. Loss': avg_val_loss,\n",
        "          'Training Time': training_time,\n",
        "          'Validation Time': validation_time\n",
        "      }\n",
        "  )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "id": "Te7OkY9szDk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8JisHdjU79u"
      },
      "outputs": [],
      "source": [
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_gpt2_5epochs_more_data/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09EpsO32nIIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats = []\n",
        "avg_train_loss = [3.15, 2.31, 2.12, 2.04, 2.01]\n",
        "avg_val_loss = [2.32, 2.10, 2.02, 1.99, 1.99]\n",
        "avg_train_loss = [1.65, 1.09, 0.78, 0.63, 0.58]\n",
        "avg_val_loss = [1.27, 0.98, 0.82, 0.75, 0.74]\n",
        "for epoch in range(0, 5):\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss[epoch],\n",
        "            'Valid. Loss': avg_val_loss[epoch]\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "12ohJmC2nISn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6DJtSzdUxTN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Display floats with two decimal places.\n",
        "# pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-IIUtxmCMmL"
      },
      "outputs": [],
      "source": [
        "with open(\"model_gpt2_10epochs_stats.pickle\", \"wb\") as f:\n",
        "  pickle.dump(training_stats, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8sE_WQoU4sa"
      },
      "outputs": [],
      "source": [
        "# Use plot styling from seaborn.\n",
        "# sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "# sns.set(font_scale=1.5)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (6,3)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-y2za2AU-ec"
      },
      "outputs": [],
      "source": [
        "def nucleus_sampling(input_ids, token_type_ids, input_len):\n",
        "  output_ids = []\n",
        "  max_len = input_len + 20\n",
        "  for pos in range(input_len, max_len):\n",
        "    output_logits = model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos-1]\n",
        "    output = F.softmax(output_logits, dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "mname = \"model_gpt2_5epochs\"\n",
        "model = GPT2LMHeadModel.from_pretrained(mname)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(mname)"
      ],
      "metadata": {
        "id": "TEyJ_dOpWV97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nucleus_sampling(input_ids, token_type_ids, input_len):\n",
        "  output_ids = []\n",
        "  top_p = 0.9\n",
        "  for pos in range(input_len, input_len + 20):\n",
        "    output = model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos-1]  # (1, V)\n",
        "    output = F.softmax(output, dim=-1)  # (1, V)\n",
        "\n",
        "    sorted_probs, sorted_idxs = torch.sort(output, descending=True)\n",
        "    cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
        "    idx_remove = cumsum_probs > top_p\n",
        "    idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
        "    idx_remove[:, 0] = False\n",
        "    sorted_probs[idx_remove] = 0.0\n",
        "    sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)\n",
        "\n",
        "    probs = torch.zeros(output.shape).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)\n",
        "    idx = torch.multinomial(probs, 1)  # (1, 1)\n",
        "\n",
        "    idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
        "    output_ids.append(idx_item)\n",
        "\n",
        "    if idx_item == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "    input_ids = torch.cat((input_ids, idx), dim=-1)\n",
        "    next_type_id = torch.LongTensor([[tokenizer.additional_special_tokens_ids[2]]])\n",
        "    token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
        "    assert input_ids.shape == token_type_ids.shape\n",
        "\n",
        "  return output_ids"
      ],
      "metadata": {
        "id": "1mfDrS0Q6otx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat\n",
        "bos = tokenizer.bos_token\n",
        "eos = tokenizer.eos_token\n",
        "kw_token = tokenizer.additional_special_tokens[0]\n",
        "s1 = tokenizer.additional_special_tokens[1]\n",
        "s2 = tokenizer.additional_special_tokens[2]\n",
        "\n",
        "keywords = [[\"cat\", \"park\", \"lunch\", \"piano\", \"sad\"],[\"dog\"],[\"love\"],[\"play\",\"basketball\"],[\"game\"]]\n",
        "conversation_history = [\"What did you do today?\"]\n",
        "print(\"Bot: What did you do today?\")\n",
        "i = 0\n",
        "input_ids_history = []\n",
        "model.to('cpu')\n",
        "while True:\n",
        "  user_input = input(\">> User: \")\n",
        "  input_ids = tokenizer.encode(s1 + user_input, add_special_tokens=False)\n",
        "  input_ids_history.append(input_ids)\n",
        "\n",
        "  keywords = kw_token + kw_token.join(keywords[i])\n",
        "  encoded_kw = tokenizer.encode(keywords, add_special_tokens=False)\n",
        "\n",
        "  input_ids = encoded_kw + [tokenizer.bos_token_id] + list(chain.from_iterable(input_ids_history)) + [tokenizer.additional_special_tokens_ids[2]]\n",
        "\n",
        "  token_type_ids = [[0] * len(hist) if h % 2 == 0 else [1] * len(hist) for h, hist in enumerate(input_ids_history)]\n",
        "  token_type_ids = [1] * len(encoded_kw) + [0] + list(chain.from_iterable(token_type_ids)) + [1]\n",
        "  input_len = len(input_ids)\n",
        "\n",
        "  print(len(input_ids), len(token_type_ids))\n",
        "  assert len(input_ids) == len(token_type_ids)\n",
        "\n",
        "  input_ids = torch.LongTensor(input_ids).unsqueeze(0)\n",
        "  token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0)\n",
        "\n",
        "  output_ids = nucleus_sampling(input_ids, token_type_ids, input_len)\n",
        "\n",
        "  res = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "  print(f\"Bot: {res}\")\n",
        "  input_ids_history.append(tokenizer.encode(s2 + res))\n",
        "\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "dozo1TqUzce1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}